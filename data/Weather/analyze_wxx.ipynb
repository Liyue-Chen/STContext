{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对收集的数据的收集和简单处理完毕，接下来进行进一步处理和分析  \n",
    "（收集和简单处理的代码，详见pa_wxx.ipynb）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 文件与文件夹说明：\n",
    "> - preprocessed: 预处理后的数据\n",
    "> - raw: 仅将时间戳调整为不同地区的时间的几乎原始的数据\n",
    "> - statistics: 统计与分析\n",
    "> - ipynb文件为代码文件，目前主要为爬取和数据分析的代码，其中有较多注释\n",
    "\n",
    "\n",
    "数据字典，目前为中文，以方便查看，后续会翻译成英文\n",
    "- key: 观测站点的唯一标识符 - (str)\n",
    "- class: 观测的类别 - (str)\n",
    "- expire_time_gmt: 观测的过期时间 - (DATE)\n",
    "- obs_id: 观测的ID - (str)\n",
    "- obs_name: 观测站点的名称 - (str)\n",
    "- valid_time_gmt: 观测的有效时间 - (DATE)\n",
    "- day_ind: 一天中的时间段（白天或晚上） - (str)\n",
    "- temp: 温度（华氏度） - (float)\n",
    "- wx_icon: 天气图标的代码 - (int)\n",
    "- icon_extd: 天气图标的扩展代码 - (int)\n",
    "- wx_phrase: 天气短语描述 - (str)\n",
    "- pressure_tend: 气压变化趋势 - (int, 0-稳定, 1-上升, 2-下降)\n",
    "- pressure_desc: 气压描述 - (str, 其实就是前一字段的描述)\n",
    "- dewPt: 露点温度（华氏度） - (float)\n",
    "- heat_index: 酷热指数 - (float)\n",
    "- rh: 相对湿度 - (float)\n",
    "- pressure: 气压 - (float)\n",
    "- vis: 能见度（英里） - (int, 0-10)\n",
    "- wc: 风寒指数 - (float)\n",
    "- wdir: 风向（角度） - (int, 以北, 顺时针的角度[10-360], 360代表北) \n",
    "- wdir_cardinal: 风向的助记符号 - (str, 描述风向, 比如N代表北风)\n",
    "- gust: 风速瞬时值（英里/小时） - (float)\n",
    "- wspd: 风速（英里/小时） - (float)\n",
    "- max_temp: 最高温度（华氏度） - (float)\n",
    "- min_temp: 最低温度（华氏度） - (float)\n",
    "- precip_total: 总降水量（英寸） - (float)\n",
    "- precip_hrly: 每小时降水量（英寸） - (float)\n",
    "- snow_hrly: 每小时降雪量（英寸） - (float)\n",
    "- uv_desc: 紫外线指数描述 - (str)\n",
    "- feels_like: 体感温度（华氏度） - (float)\n",
    "- uv_index: 紫外线指数 https://www.epa.gov/sunsafety/calculating-uv-index-0 - (float)\n",
    "\n",
    "(以下列除了clds，都无数据，只是爬取时有该列，但是并没有数据)\n",
    "- qualifier: 气象数据的限定条件\n",
    "- qualifier_svrty: 限定条件的严重程度\n",
    "- blunt_phrase: 概括的天气短语描述\n",
    "- terse_phrase: 简短的天气短语描述\n",
    "- clds: 云量 https://www.eoas.ubc.ca/courses/atsc113/flying/met_concepts/01-met_concepts/01c-cloud_coverage/index.html\n",
    "- water_temp: 水温\n",
    "- primary_wave_period: 主波周期\n",
    "- primary_wave_height: 主波高度\n",
    "- primary_swell_period: 主涌浪周期\n",
    "- primary_swell_height: 主涌浪高度\n",
    "- primary_swell_direction: 主涌浪方向\n",
    "- secondary_swell_period: 次涌浪周期\n",
    "- secondary_swell_height: 次涌浪高度\n",
    "- secondary_swell_direction: 次涌浪方向"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据非空率、分布情况（词频率）、异常值比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "raw_data_dir = 'raw'\n",
    "filename = 'DCWeather_ori_1.csv'\n",
    "df = pd.read_csv(os.path.join(raw_data_dir,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NA rate of column key is 0.0\n",
      "NA rate of column class is 0.0\n",
      "NA rate of column expire_time_gmt is 0.0\n",
      "NA rate of column obs_id is 0.0\n",
      "NA rate of column obs_name is 0.0\n",
      "NA rate of column valid_time_gmt is 0.0\n",
      "NA rate of column day_ind is 0.0\n",
      "NA rate of column temp is 4.2222597534200305e-05\n",
      "NA rate of column wx_icon is 6.333389630130046e-05\n",
      "NA rate of column icon_extd is 6.333389630130046e-05\n",
      "NA rate of column wx_phrase is 6.333389630130046e-05\n",
      "NA rate of column pressure_tend is 0.7401199121769971\n",
      "NA rate of column pressure_desc is 0.7401199121769971\n",
      "NA rate of column dewPt is 0.002934470528626921\n",
      "NA rate of column heat_index is 0.0036522546867083262\n",
      "NA rate of column rh is 4.2222597534200305e-05\n",
      "NA rate of column pressure is 0.00012666779260260091\n",
      "NA rate of column vis is 0.00010555649383550076\n",
      "NA rate of column wc is 0.0013300118223273096\n",
      "NA rate of column wdir is 0.10133423408208073\n",
      "NA rate of column wdir_cardinal is 0.0007177841580814051\n",
      "NA rate of column gust is 0.8782933626076677\n",
      "NA rate of column wspd is 0.01798682654956933\n",
      "NA rate of column max_temp is 0.9462717446377301\n",
      "NA rate of column min_temp is 0.9465673028204695\n",
      "NA rate of column precip_total is 0.9909854754264482\n",
      "NA rate of column precip_hrly is 0.1244088836345212\n",
      "NA rate of column snow_hrly is 0.9988811011653437\n",
      "NA rate of column uv_desc is 0.07515622361087654\n",
      "NA rate of column feels_like is 0.0008444519506840061\n",
      "NA rate of column uv_index is 0.0\n",
      "NA rate of column qualifier is 1.0\n",
      "NA rate of column qualifier_svrty is 1.0\n",
      "NA rate of column blunt_phrase is 1.0\n",
      "NA rate of column terse_phrase is 1.0\n",
      "NA rate of column clds is 0.00010555649383550076\n",
      "NA rate of column water_temp is 1.0\n",
      "NA rate of column primary_wave_period is 1.0\n",
      "NA rate of column primary_wave_height is 1.0\n",
      "NA rate of column primary_swell_period is 1.0\n",
      "NA rate of column primary_swell_height is 1.0\n",
      "NA rate of column primary_swell_direction is 1.0\n",
      "NA rate of column secondary_swell_period is 1.0\n",
      "NA rate of column secondary_swell_height is 1.0\n",
      "NA rate of column secondary_swell_direction is 1.0\n"
     ]
    }
   ],
   "source": [
    "na_rate = {}\n",
    "for column in columns:\n",
    "    na_rate[column] = sum(pd.isna(df[column]))/df.shape[0]\n",
    "    print('NA rate of column {} is {}'.format(column,sum(pd.isna(df[column]))/df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 # NA rate lower than threshold we will take further action\n",
    "target_column_list = []\n",
    "for key,value in zip(na_rate.keys(),na_rate.values()):\n",
    "    if value < threshold:\n",
    "        target_column_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[target_column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KDCA    47368\nName: key, dtype: int64\nobservation    47368\nName: class, dtype: int64\n2013-11-03 01:52:00    2\n2016-11-06 01:52:00    2\n2015-11-01 01:52:00    2\n2014-11-02 01:52:00    2\n2015-12-26 09:28:00    1\n                      ..\n2016-12-17 05:52:00    1\n2014-04-30 16:07:00    1\n2014-11-19 22:52:00    1\n2013-07-15 11:52:00    1\n2015-06-24 20:52:00    1\nName: expire_time_gmt, Length: 47364, dtype: int64\nKDCA    47368\nName: obs_id, dtype: int64\nWashington/Natl    47368\nName: obs_name, dtype: int64\n2015-11-01 01:52:00    2\n2014-11-02 01:52:00    2\n2013-11-03 01:52:00    2\n2016-11-06 01:52:00    2\n2017-04-01 05:52:00    1\n                      ..\n2015-10-08 12:34:00    1\n2017-02-12 11:52:00    1\n2016-07-19 23:04:00    1\n2013-08-15 12:52:00    1\n2015-06-24 20:52:00    1\nName: valid_time_gmt, Length: 47364, dtype: int64\nD    25428\nN    21940\nName: day_ind, dtype: int64\n73.0     1419\n75.0     1357\n72.0     1194\n77.0     1131\n79.0     1072\n         ... \n99.0        2\n6.0         2\n100.0       2\nNaN         2\n118.0       1\nName: temp, Length: 97, dtype: int64\n26.0    14526\n28.0     6569\n33.0     4576\n27.0     4181\n11.0     3931\n30.0     3838\n34.0     2816\n29.0     2540\n9.0       926\n4.0       684\n14.0      624\n12.0      615\n20.0      515\n40.0      329\n38.0      146\n21.0      130\n7.0       126\n16.0       73\n47.0       70\n5.0        43\n10.0       42\n6.0        27\n42.0       25\n18.0       10\nNaN         3\n8.0         2\n24.0        1\nName: wx_icon, dtype: int64\n2600.0    14237\n2800.0     6383\n3300.0     4514\n2700.0     4131\n1201.0     3803\n          ...  \n1010.0        1\n2470.0        1\n3899.0        1\n1000.0        1\n1891.0        1\nName: icon_extd, Length: 66, dtype: int64\nCloudy                          14237\nMostly Cloudy                   10514\nFair                             7215\nPartly Cloudy                    6175\nLight Rain                       3803\nLight Drizzle                     920\nRain                              596\nLight Snow                        570\nFog                               423\nHeavy Rain                        304\nLight Rain with Thunder           301\nCloudy / Windy                    289\nMostly Cloudy / Windy             236\nPartly Cloudy / Windy             203\nHeavy T-Storm                     190\nFair / Windy                      177\nThunder                           147\nHaze                              130\nLight Rain / Windy                128\nT-Storm                           113\nWintry Mix                        100\nMist                               67\nSnow                               55\nLight Snow / Windy                 54\nHeavy T-Storm / Windy              54\nThunder in the Vicinity            53\nRain and Snow                      43\nLight Freezing Rain                40\nRain and Sleet                     27\nHeavy Rain / Windy                 25\nRain / Windy                       19\nT-Storm / Windy                    19\nSnow / Windy                       18\nHeavy Snow                         18\nThunder / Windy                    16\nPatches of Fog                     15\nShallow Fog                        10\nWintry Mix / Windy                 10\nLight Snow and Sleet                8\nHeavy Snow / Windy                  7\nSnow and Sleet                      5\nSleet                               4\nLight Drizzle / Windy               4\nThunder and Hail                    4\nNaN                                 3\nLight Snow and Sleet / Windy        3\nThunder and Hail / Windy            3\nLight Sleet                         3\nHeavy Sleet                         2\nDrizzle and Fog                     2\nLight Freezing Drizzle              2\nRain / Freezing Rain                1\nFreezing Rain                       1\nSqualls / Windy                     1\nLight Sleet / Windy                 1\nName: wx_phrase, dtype: int64\n 66.0    1418\n 68.0    1295\n 70.0    1264\n 72.0    1246\n 64.0    1221\n         ... \n-14.0       8\n 80.0       6\n-13.0       5\n-12.0       4\n-15.0       1\nName: dewPt, Length: 97, dtype: int64\n73.0     1419\n75.0     1357\n72.0     1194\n74.0     1053\n70.0     1027\n         ... \n9.0         4\n110.0       4\n112.0       3\n6.0         2\n114.0       2\nName: heat_index, Length: 109, dtype: int64\n93.0    2004\n87.0    1626\n81.0    1144\n83.0    1100\n82.0    1099\n        ... \n15.0       7\n13.0       4\nNaN        2\n14.0       2\n7.0        1\nName: rh, Length: 87, dtype: int64\n29.94    1024\n29.98    1019\n30.01    1017\n30.03    1006\n29.99     995\n         ... \n29.05       1\n30.79       1\n29.08       1\n30.80       1\n29.18       1\nName: pressure, Length: 172, dtype: int64\n10.00    37140\n2.00      1446\n4.00      1194\n3.00      1118\n5.00      1063\n7.00      1013\n1.00       995\n9.00       983\n6.00       953\n8.00       799\n0.75       240\n0.50       198\n0.25       142\n0.12        60\n0.06        16\nNaN          5\n0.00         3\nName: vis, dtype: int64\n 73.0     1419\n 75.0     1355\n 72.0     1194\n 77.0     1129\n 79.0     1072\n          ... \n-10.0        3\n 99.0        2\n 100.0       2\n-11.0        1\n 118.0       1\nName: wc, Length: 114, dtype: int64\nNaN      4800\n190.0    3149\n180.0    2921\n170.0    2496\n200.0    2307\n340.0    1989\n330.0    1975\n320.0    1838\n350.0    1791\n30.0     1544\n40.0     1538\n210.0    1472\n50.0     1446\n20.0     1299\n160.0    1208\n60.0     1193\n310.0    1184\n360.0    1134\n10.0     1051\n300.0    1027\n220.0     949\n290.0     854\n70.0      845\n150.0     718\n230.0     693\n280.0     624\n80.0      598\n140.0     535\n240.0     501\n130.0     498\n270.0     495\n90.0      494\n110.0     456\n120.0     444\n100.0     441\n260.0     436\n250.0     425\nName: wdir, dtype: int64\nS       8566\nCALM    4140\nN       3976\nNNW     3964\nSSW     3779\nNW      3022\nNE      2984\nNNE     2843\nENE     2038\nSSE     1926\nWNW     1881\nSW      1642\nW       1555\nE       1533\nSE      1033\nWSW      926\nESE      900\nVAR      626\nNaN       34\nName: wdir_cardinal, dtype: int64\n7.0     4880\n6.0     4648\n8.0     4534\n5.0     4364\n9.0     4118\n3.0     3765\n10.0    3619\n0.0     3322\n12.0    2971\n13.0    2503\n14.0    1959\n15.0    1418\n16.0    1148\nNaN      852\n17.0     810\n18.0     666\n20.0     492\n21.0     379\n22.0     265\n23.0     178\n24.0     160\n25.0     103\n26.0      65\n28.0      39\n29.0      31\n30.0      25\n31.0      15\n32.0      15\n37.0       5\n33.0       4\n36.0       4\n35.0       4\n38.0       2\n39.0       2\n41.0       1\n45.0       1\n52.0       1\nName: wspd, dtype: int64\n0.00    35305\nNaN      5893\n0.01     1690\n0.02      901\n0.03      633\n        ...  \n0.67        1\n1.09        1\n1.25        1\n0.82        1\n1.11        1\nName: precip_hrly, Length: 100, dtype: int64\nLow          37465\nModerate      5291\nNaN           3560\nHigh           969\nVery High       83\nName: uv_desc, dtype: int64\n 73.0     1419\n 75.0     1357\n 72.0     1194\n 74.0     1053\n 70.0     1027\n          ... \n-9.0         3\n-10.0        3\n-6.0         3\n 114.0       2\n-11.0        1\nName: feels_like, Length: 126, dtype: int64\n 0      23597\n 1      10060\n 2       3808\n 3       2526\n 4       1744\n        ...  \n-311        1\n-498        1\n-362        1\n-403        1\n-434        1\nName: uv_index, Length: 517, dtype: int64\nOVC    21217\nBKN    12104\nSCT     6137\nFEW     4462\nCLR     3443\nNaN        5\nName: clds, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in target_column_list:\n",
    "    print(df_1[column].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.00    35305\nNaN      5893\n0.01     1690\n0.02      901\n0.03      633\n        ...  \n0.67        1\n1.09        1\n1.25        1\n0.82        1\n1.11        1\nName: precip_hrly, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_1['precip_hrly'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.loc[df_1['valid_time_gmt']=='2016-07-24 13:53:00',weather_info.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "37248"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from datetime import timedelta\n",
    "td = timedelta(hours=1)\n",
    "(parse('2017-09-30')-parse('2013-07-01'))//td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['valid_dt'] = df_1['valid_time_gmt'].apply(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexs = pd.date_range(start='2013-07-01 00:00:00',periods=4344,freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0039\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0140\n",
      "Duration:0.0060\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0039\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0031\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0039\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0036\n",
      "Duration:0.0024\n",
      "Duration:0.0026\n",
      "Duration:0.0022\n",
      "Duration:0.0027\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0036\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0026\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0023\n",
      "Duration:0.0034\n",
      "Duration:0.0036\n",
      "Duration:0.0027\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0028\n",
      "Duration:0.0025\n",
      "Duration:0.0030\n",
      "Duration:0.0035\n",
      "Duration:0.0020\n",
      "Duration:0.0038\n",
      "Duration:0.0045\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0032\n",
      "Duration:0.0032\n",
      "Duration:0.0035\n",
      "Duration:0.0032\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0028\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0033\n",
      "Duration:0.0030\n",
      "Duration:0.0022\n",
      "Duration:0.0028\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0033\n",
      "Duration:0.0025\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0028\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0033\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0027\n",
      "Duration:0.0031\n",
      "Duration:0.0019\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0040\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0031\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0019\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0041\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0025\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0027\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0039\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0033\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0037\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0031\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0050\n",
      "Duration:0.0046\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0026\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0019\n",
      "Duration:0.0031\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0026\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0021\n",
      "Duration:0.0019\n",
      "Duration:0.0040\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0031\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0031\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0039\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0019\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0040\n",
      "Duration:0.0040\n",
      "Duration:0.0040\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0019\n",
      "Duration:0.0020\n",
      "Duration:0.0021\n",
      "Duration:0.0020\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0029\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0021\n",
      "Duration:0.0029\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n",
      "Duration:0.0030\n",
      "Duration:0.0020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "td = timedelta(minutes=30)\n",
    "import time\n",
    "df_list = []\n",
    "for time_index in time_indexs:\n",
    "    t_1 = time.time()\n",
    "    datetime_str = str(time_index)\n",
    "    dt = parse(datetime_str)\n",
    "    dt_start = dt - td\n",
    "    dt_end = dt + td\n",
    "    df_tmp = df_1.loc[(df_1['valid_dt']>dt_start)&(df_1['valid_dt']<dt_end),:]\n",
    "    df_tmp = df_tmp.sort_values('valid_dt')\n",
    "    l,_ = df_tmp.shape\n",
    "    if l>0:\n",
    "        df_list.append(pd.DataFrame(df_tmp.iloc[l//2,:].values.reshape([1,-1]),columns=df_1.columns))\n",
    "    else:\n",
    "        df_list.append(pd.DataFrame(data=dict(zip(df_1.columns,[[np.nan] for column in df_1.columns]))))\n",
    "    t_2 = time.time()\n",
    "    print('Duration:%.4f'%(t_2-t_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "time_index = time_indexs[1]\n",
    "t_1 = time.time()\n",
    "datetime_str = str(time_index)\n",
    "dt = parse(datetime_str)\n",
    "dt_start = dt - td\n",
    "dt_end = dt + td\n",
    "df_tmp = df_1.loc[(df_1['valid_dt']>dt_start)&(df_1['valid_dt']<dt_end),:]\n",
    "df_tmp = df_tmp.sort_values('valid_dt')\n",
    "l,_ = df_tmp.shape\n",
    "if l>0:\n",
    "    df_list.append(pd.DataFrame(df_tmp.iloc[l//2,:].values.reshape([1,-1]),columns=df_1.columns))\n",
    "else:\n",
    "    df_list.append(pd.DataFrame(data=dict(zip(df_1.columns,[[np.nan] for column in df_1.columns]))))\n",
    "t_2 = time.time()\n",
    "print('Duration:%.4f'%(t_2-t_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat(df_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.fillna(method='ffill',inplace=True)\n",
    "df_new.fillna(method='bfill',inplace=True)\n",
    "df_new.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wx_phrase(x):\n",
    "    result = 'Others'\n",
    "    if 'Fair' in x:\n",
    "        result = 'Fair'\n",
    "    if 'Windy' in x:\n",
    "        result = 'Windy'\n",
    "    if 'Cloud' in x:\n",
    "        result = 'Cloud'\n",
    "    if ('Snow' in x) or ('Sleet' in x):\n",
    "        result = 'Snow'\n",
    "    if ('Thunder' in x) or ('Storm' in x):\n",
    "        result = 'Thunderstorm'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['wx_phrase'] = df_new['wx_phrase'].apply(convert_wx_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OVC    1429\n",
       "BKN    1217\n",
       "SCT     747\n",
       "FEW     486\n",
       "CLR     465\n",
       "Name: clds, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_new['clds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset = {}\n",
    "time_range = None\n",
    "time_fitness = None\n",
    "weather_value = None\n",
    "weather_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_info = {\n",
    "    'temp':'float',\n",
    "    'wx_icon':'int',\n",
    "    'wx_phrase':'str',\n",
    "    'dewPt':'float',\n",
    "    'heat_index':'float',\n",
    "    'rh':'float',\n",
    "    'pressure':'float',\n",
    "    'vis':'float',\n",
    "    'wc':'float',\n",
    "    'wdir_cardinal':'str',\n",
    "    'wspd':'float',\n",
    "    'uv_desc':'str',\n",
    "    'feels_like':'float',\n",
    "    'uv_index':'float',\n",
    "    'clds':'str'\n",
    "}\n",
    "time_range = ['2013-07-01','2017-09-30']\n",
    "time_fitness = 60\n",
    "weather_value = df_new[weather_info.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset['TimeRange'] = time_range\n",
    "weather_dataset['TimeFitness'] = time_fitness\n",
    "weather_dataset['WeatherValue'] = weather_value\n",
    "weather_dataset['WeatherInfo'] = weather_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('Weather_DC.pkl','wb') as fp:\n",
    "    pkl.dump(weather_dataset,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_list = list(map(lambda x: 'Fair' if 'Fair' in x else 'Others',list(df_1['wx_phrase'].unique())))\n",
    "list(map(lambda x: 'Rain' if ('Rain' in x) or ('Drizzle' in x) else 'Others',list(df_1['wx_phrase'].unique())))\n",
    "list(map(lambda x: 'Cloud' if 'Cloud' in x else 'Others',list(df_1['wx_phrase'].unique())))\n",
    "list(map(lambda x: 'Snow' if ('Snow' in x) or ('Sleet' in x) else 'Others',list(df_1['wx_phrase'].unique())))\n",
    "list(map(lambda x: 'Windy' if ('Windy' in x) else 'Others',list(df_1['wx_phrase'].unique())))\n",
    "list(map(lambda x: 'Thunderstorm' if ('Thunder' in x) or ('Storm' in x) else 'Others',list(df_1['wx_phrase'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['wx_phrase'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = True\n",
    "1 if x else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nonempty_rate_dict={}\n",
    "included_column_name_list = []\n",
    "for column in df.columns:\n",
    "    nonempty_rate_dict[column] = np.sum(~pd.isna(df[column]))/df.shape[0]\n",
    "    if nonempty_rate_dict[column] > 0:\n",
    "        included_column_name_list.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~pd.isna(df['pressure_tend']),included_column_name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_time_gmt = list(df['valid_time_gmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "# from tqdm import tqdm\n",
    "\n",
    "cities_ls = ['BAY','Chicago','DC','LA','Melbourne','NYC']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "# col = ['STATION', 'STATION_NAME', 'ELEVATION', 'LATITUDE', 'LONGITUDE', 'DATE', 'REPORTTYPE', 'HOURLYSKYCONDITIONS', 'HOURLYVISIBILITY', 'HOURLYPRSENTWEATHERTYPE', 'HOURLYDRYBULBTEMPF', 'HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPF', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempF', 'HOURLYDewPointTempC', 'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYWindGustSpeed', 'HOURLYStationPressure', 'HOURLYPressureTendency', 'HOURLYPressureChange', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting', 'DAILYMaximumDryBulbTemp', 'DAILYMinimumDryBulbTemp', 'DAILYAverageDryBulbTemp', 'DAILYDeptFromNormalAverageTemp', 'DAILYAverageRelativeHumidity', 'DAILYAverageDewPointTemp', 'DAILYAverageWetBulbTemp', 'DAILYHeatingDegreeDays', 'DAILYCoolingDegreeDays', 'DAILYSunrise', 'DAILYSunset', 'DAILYWeather', 'DAILYPrecip', 'DAILYSnowfall', 'DAILYSnowDepth', 'DAILYAverageStationPressure', 'DAILYAverageSeaLevelPressure', 'DAILYAverageWindSpeed', 'DAILYPeakWindSpeed', 'PeakWindDirection', 'DAILYSustainedWindSpeed', 'DAILYSustainedWindDirection', 'MonthlyMaximumTemp', 'MonthlyMinimumTemp', 'MonthlyMeanTemp', 'MonthlyAverageRH', 'MonthlyDewpointTemp', 'MonthlyWetBulbTemp', 'MonthlyAvgHeatingDegreeDays', 'MonthlyAvgCoolingDegreeDays', 'MonthlyStationPressure', 'MonthlySeaLevelPressure', 'MonthlyAverageWindSpeed', 'MonthlyTotalSnowfall', 'MonthlyDeptFromNormalMaximumTemp', 'MonthlyDeptFromNormalMinimumTemp', 'MonthlyDeptFromNormalAverageTemp', 'MonthlyDeptFromNormalPrecip', 'MonthlyTotalLiquidPrecip', 'MonthlyGreatestPrecip', 'MonthlyGreatestPrecipDate', 'MonthlyGreatestSnowfall', 'MonthlyGreatestSnowfallDate', 'MonthlyGreatestSnowDepth', 'MonthlyGreatestSnowDepthDate', 'MonthlyDaysWithGT90Temp', 'MonthlyDaysWithLT32Temp', 'MonthlyDaysWithGT32Temp', 'MonthlyDaysWithLT0Temp', 'MonthlyDaysWithGT001Precip', 'MonthlyDaysWithGT010Precip', 'MonthlyDaysWithGT1Snow', 'MonthlyMaxSeaLevelPressureValue', 'MonthlyMaxSeaLevelPressureDate', 'MonthlyMaxSeaLevelPressureTime', 'MonthlyMinSeaLevelPressureValue', 'MonthlyMinSeaLevelPressureDate', 'MonthlyMinSeaLevelPressureTime', 'MonthlyTotalHeatingDegreeDays', 'MonthlyTotalCoolingDegreeDays', 'MonthlyDeptFromNormalHeatingDD', 'MonthlyDeptFromNormalCoolingDD', 'MonthlyTotalSeasonToDateHeatingDD', 'MonthlyTotalSeasonToDateCoolingDD']\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "# 适合分析和画图的列\n",
    "draw_col = ['temp', 'wx_phrase', 'pressure_tend', 'pressure_desc',\\\n",
    "            'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc',\\\n",
    "            'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp',\\\n",
    "            'min_temp', 'precip_total', 'precip_hrly', 'snow_hrly',\\\n",
    "            'uv_desc', 'feels_like', 'uv_index', 'clds', ]\n",
    "\n",
    "for c in cities_ls:\n",
    "    f_str = c + suffix # the name of the file\n",
    "    df_statistics = pd.read_csv(f_str)\n",
    "    num_rows = len(df_statistics)\n",
    "    print(c, \"Number of rows:\", num_rows)\n",
    "\n",
    "    # 统计一下数据的非空率（即 1 减去 缺失率 ）\n",
    "    prop_non_null_df = pd.DataFrame(columns=col)\n",
    "    for col_name in col:\n",
    "        prop_non_null = df_statistics[col_name].count() / num_rows\n",
    "        prop_non_null_df.loc[0, col_name] = prop_non_null\n",
    "        \n",
    "    prop_non_null_path = f'./statistics/{c}/'\n",
    "    prop_non_null_file = c+'Weather_ori_1_prop_non_null.csv'\n",
    "    if not os.path.exists(prop_non_null_path):\n",
    "        os.makedirs(prop_non_null_path)\n",
    "    prop_non_null_df.to_csv(prop_non_null_path+prop_non_null_file,\\\n",
    "                          index=False)\n",
    "    \n",
    "    # 统计一下分布情况\n",
    "    for col_name in draw_col:\n",
    "        distribution = df_statistics[col_name].value_counts() / num_rows\n",
    "        if len(distribution) == 0: continue\n",
    "        distribution_path = f'./statistics/{c}/distribution/'\n",
    "        if not os.path.exists(distribution_path):\n",
    "            os.makedirs(distribution_path)\n",
    "        distribution_file = c+f'Weather_ori_1_{col_name}.csv'\n",
    "        distribution.to_csv(distribution_path+distribution_file)\n",
    "        \n",
    "        distribution.sort_index(inplace=True) # 先根据索引值排序\n",
    "        # 区域图\n",
    "        distribution.plot.area(x=distribution.index, stacked=False)\n",
    "\n",
    "        # 折线图\n",
    "        # distribution.plot.line()\n",
    "\n",
    "        # 柱状图\n",
    "        # distribution.plot.bar()\n",
    "\n",
    "        plt.grid(color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "        if col_name == 'wx_phrase': plt.xticks(rotation=15)\n",
    "        plt.savefig((distribution_path+distribution_file)[:-4], dpi=300)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 将所有城市的分布统计到同一张图里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "cities_ls = ['BAY','Chicago','DC','LA','Melbourne','NYC']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "# col = ['STATION', 'STATION_NAME', 'ELEVATION', 'LATITUDE', 'LONGITUDE', 'DATE', 'REPORTTYPE', 'HOURLYSKYCONDITIONS', 'HOURLYVISIBILITY', 'HOURLYPRSENTWEATHERTYPE', 'HOURLYDRYBULBTEMPF', 'HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPF', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempF', 'HOURLYDewPointTempC', 'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYWindGustSpeed', 'HOURLYStationPressure', 'HOURLYPressureTendency', 'HOURLYPressureChange', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting', 'DAILYMaximumDryBulbTemp', 'DAILYMinimumDryBulbTemp', 'DAILYAverageDryBulbTemp', 'DAILYDeptFromNormalAverageTemp', 'DAILYAverageRelativeHumidity', 'DAILYAverageDewPointTemp', 'DAILYAverageWetBulbTemp', 'DAILYHeatingDegreeDays', 'DAILYCoolingDegreeDays', 'DAILYSunrise', 'DAILYSunset', 'DAILYWeather', 'DAILYPrecip', 'DAILYSnowfall', 'DAILYSnowDepth', 'DAILYAverageStationPressure', 'DAILYAverageSeaLevelPressure', 'DAILYAverageWindSpeed', 'DAILYPeakWindSpeed', 'PeakWindDirection', 'DAILYSustainedWindSpeed', 'DAILYSustainedWindDirection', 'MonthlyMaximumTemp', 'MonthlyMinimumTemp', 'MonthlyMeanTemp', 'MonthlyAverageRH', 'MonthlyDewpointTemp', 'MonthlyWetBulbTemp', 'MonthlyAvgHeatingDegreeDays', 'MonthlyAvgCoolingDegreeDays', 'MonthlyStationPressure', 'MonthlySeaLevelPressure', 'MonthlyAverageWindSpeed', 'MonthlyTotalSnowfall', 'MonthlyDeptFromNormalMaximumTemp', 'MonthlyDeptFromNormalMinimumTemp', 'MonthlyDeptFromNormalAverageTemp', 'MonthlyDeptFromNormalPrecip', 'MonthlyTotalLiquidPrecip', 'MonthlyGreatestPrecip', 'MonthlyGreatestPrecipDate', 'MonthlyGreatestSnowfall', 'MonthlyGreatestSnowfallDate', 'MonthlyGreatestSnowDepth', 'MonthlyGreatestSnowDepthDate', 'MonthlyDaysWithGT90Temp', 'MonthlyDaysWithLT32Temp', 'MonthlyDaysWithGT32Temp', 'MonthlyDaysWithLT0Temp', 'MonthlyDaysWithGT001Precip', 'MonthlyDaysWithGT010Precip', 'MonthlyDaysWithGT1Snow', 'MonthlyMaxSeaLevelPressureValue', 'MonthlyMaxSeaLevelPressureDate', 'MonthlyMaxSeaLevelPressureTime', 'MonthlyMinSeaLevelPressureValue', 'MonthlyMinSeaLevelPressureDate', 'MonthlyMinSeaLevelPressureTime', 'MonthlyTotalHeatingDegreeDays', 'MonthlyTotalCoolingDegreeDays', 'MonthlyDeptFromNormalHeatingDD', 'MonthlyDeptFromNormalCoolingDD', 'MonthlyTotalSeasonToDateHeatingDD', 'MonthlyTotalSeasonToDateCoolingDD']\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "# 适合分析和画图的列\n",
    "draw_col = ['temp', 'wx_phrase', 'pressure_tend', 'pressure_desc',\\\n",
    "            'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc',\\\n",
    "            'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp',\\\n",
    "            'min_temp', 'precip_total', 'precip_hrly', 'snow_hrly',\\\n",
    "            'uv_desc', 'feels_like', 'uv_index', 'clds', ]\n",
    "# 以数据为主的列，以及可能出现异常值的列\n",
    "number_col = ['temp', 'pressure_tend', 'dewPt', \\\n",
    "                'heat_index', 'rh', 'pressure', 'vis', 'wc',\\\n",
    "                'wdir', 'gust', 'wspd', 'max_temp',\\\n",
    "                'min_temp', 'precip_total', 'precip_hrly', 'snow_hrly',\\\n",
    "                'feels_like', 'uv_index', ]\n",
    "\n",
    "# 将所有城市的分布统计到同一张图里\n",
    "for col_name in tqdm(draw_col):\n",
    "    distribution_path = f'./statistics/All_cities/distribution/'\n",
    "    distribution_file = f'{col_name}.csv'\n",
    "    tmp_ls = []\n",
    "    num_cities = len(cities_ls)\n",
    "    for c in cities_ls:\n",
    "        f_str = c + suffix # the name of the file\n",
    "        df_statistics = pd.read_csv(f_str)\n",
    "        num_rows = len(df_statistics)\n",
    "        distribution = df_statistics[col_name].value_counts() / num_rows\n",
    "        distribution.sort_index(inplace=True) # 先根据索引值排序\n",
    "        tmp_ls.append(distribution)\n",
    "\n",
    "    plt.figure(figsize=(16,9), dpi=120)\n",
    "    for i,c in enumerate(cities_ls):\n",
    "        plt.subplot(math.ceil(num_cities/3), 3, i+1)\n",
    "        d = tmp_ls[i]\n",
    "        ax = d.plot.area(x=d.index, stacked=False)\n",
    "        if col_name in number_col and len(d):\n",
    "            sum = d.sum()\n",
    "            now_sum = 0\n",
    "            last_sum = 0\n",
    "            index_ls = list(d.index)\n",
    "            x_median = index_ls[0]  \n",
    "            j = 0\n",
    "            while(now_sum < sum/2):\n",
    "                now_sum += d.iloc[j]\n",
    "                if now_sum >= sum/2:\n",
    "                    diff = now_sum - last_sum\n",
    "                    oppo_diff = sum / 2 - last_sum\n",
    "                    last_index = index_ls[j-1] if j>0 else 0\n",
    "                    now_index = index_ls[j]\n",
    "                    x_median = last_index + (now_index - last_index) * oppo_diff / diff\n",
    "                    break\n",
    "                last_sum = now_sum\n",
    "                j+=1\n",
    "            ax.axvline(x=x_median, color='red')\n",
    "        plt.title(c)\n",
    "    if not os.path.exists(distribution_path):\n",
    "        os.makedirs(distribution_path)\n",
    "    plt.suptitle(col_name, fontsize=26, weight = 'extra bold')\n",
    "    plt.savefig((distribution_path+distribution_file)[:-4])\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openpyxl import Workbook\n",
    "\n",
    "cities_ls = ['BAY','Chicago','DC','LA','Melbourne','NYC']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "# col = ['STATION', 'STATION_NAME', 'ELEVATION', 'LATITUDE', 'LONGITUDE', 'DATE', 'REPORTTYPE', 'HOURLYSKYCONDITIONS', 'HOURLYVISIBILITY', 'HOURLYPRSENTWEATHERTYPE', 'HOURLYDRYBULBTEMPF', 'HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPF', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempF', 'HOURLYDewPointTempC', 'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYWindGustSpeed', 'HOURLYStationPressure', 'HOURLYPressureTendency', 'HOURLYPressureChange', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting', 'DAILYMaximumDryBulbTemp', 'DAILYMinimumDryBulbTemp', 'DAILYAverageDryBulbTemp', 'DAILYDeptFromNormalAverageTemp', 'DAILYAverageRelativeHumidity', 'DAILYAverageDewPointTemp', 'DAILYAverageWetBulbTemp', 'DAILYHeatingDegreeDays', 'DAILYCoolingDegreeDays', 'DAILYSunrise', 'DAILYSunset', 'DAILYWeather', 'DAILYPrecip', 'DAILYSnowfall', 'DAILYSnowDepth', 'DAILYAverageStationPressure', 'DAILYAverageSeaLevelPressure', 'DAILYAverageWindSpeed', 'DAILYPeakWindSpeed', 'PeakWindDirection', 'DAILYSustainedWindSpeed', 'DAILYSustainedWindDirection', 'MonthlyMaximumTemp', 'MonthlyMinimumTemp', 'MonthlyMeanTemp', 'MonthlyAverageRH', 'MonthlyDewpointTemp', 'MonthlyWetBulbTemp', 'MonthlyAvgHeatingDegreeDays', 'MonthlyAvgCoolingDegreeDays', 'MonthlyStationPressure', 'MonthlySeaLevelPressure', 'MonthlyAverageWindSpeed', 'MonthlyTotalSnowfall', 'MonthlyDeptFromNormalMaximumTemp', 'MonthlyDeptFromNormalMinimumTemp', 'MonthlyDeptFromNormalAverageTemp', 'MonthlyDeptFromNormalPrecip', 'MonthlyTotalLiquidPrecip', 'MonthlyGreatestPrecip', 'MonthlyGreatestPrecipDate', 'MonthlyGreatestSnowfall', 'MonthlyGreatestSnowfallDate', 'MonthlyGreatestSnowDepth', 'MonthlyGreatestSnowDepthDate', 'MonthlyDaysWithGT90Temp', 'MonthlyDaysWithLT32Temp', 'MonthlyDaysWithGT32Temp', 'MonthlyDaysWithLT0Temp', 'MonthlyDaysWithGT001Precip', 'MonthlyDaysWithGT010Precip', 'MonthlyDaysWithGT1Snow', 'MonthlyMaxSeaLevelPressureValue', 'MonthlyMaxSeaLevelPressureDate', 'MonthlyMaxSeaLevelPressureTime', 'MonthlyMinSeaLevelPressureValue', 'MonthlyMinSeaLevelPressureDate', 'MonthlyMinSeaLevelPressureTime', 'MonthlyTotalHeatingDegreeDays', 'MonthlyTotalCoolingDegreeDays', 'MonthlyDeptFromNormalHeatingDD', 'MonthlyDeptFromNormalCoolingDD', 'MonthlyTotalSeasonToDateHeatingDD', 'MonthlyTotalSeasonToDateCoolingDD']\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "# 适合分析和画图的列\n",
    "draw_col = ['temp', 'wx_phrase', 'pressure_tend', 'pressure_desc',\\\n",
    "            'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc',\\\n",
    "            'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp',\\\n",
    "            'min_temp', 'precip_total', 'precip_hrly', 'snow_hrly',\\\n",
    "            'uv_desc', 'feels_like', 'uv_index', 'clds', ]\n",
    "\n",
    "\n",
    "for c in cities_ls:\n",
    "    f_str = c + suffix # the name of the file\n",
    "    df_statistics = pd.read_csv(f_str)\n",
    "    num_rows = len(df_statistics)\n",
    "    print(c, \"Number of rows:\", num_rows)\n",
    "\n",
    "    outlier_ratio_df = pd.DataFrame(columns=col) # 异常值比例\n",
    "    # 以数据为主的列，以及可能出现异常值的列\n",
    "    number_col = ['temp', 'pressure_tend', 'dewPt', \\\n",
    "                    'heat_index', 'rh', 'pressure', 'vis', 'wc',\\\n",
    "                    'wdir', 'gust', 'wspd', 'max_temp',\\\n",
    "                    'min_temp', 'precip_total', 'precip_hrly', 'snow_hrly',\\\n",
    "                    'feels_like', 'uv_index', ]\n",
    "    df_2_list = [] # 超过2标准差的异常值列表\n",
    "    df_3_list = [] # 超过3标准差的异常值列表\n",
    "\n",
    "    for col_name in number_col:\n",
    "        # 1. 统计异常值（大于两个标准差）比例\n",
    "        data = df_statistics[col_name]\n",
    "        if len(data) == 0: continue\n",
    "\n",
    "        mean = data.mean()\n",
    "        std = data.std()\n",
    "\n",
    "        raw_out = (data - mean).abs() / std\n",
    "        mask_2 = raw_out > 2 # 偏离均值大于两个标准差\n",
    "        mask_3 = raw_out > 3 # 偏离均值大于三个标准差\n",
    "\n",
    "        outlier_ratio_2 = mask_2.mean()\n",
    "        outlier_ratio_3 = mask_3.mean()\n",
    "        outlier_ratio_df.loc['std', col_name] = std\n",
    "        outlier_ratio_df.loc['ratio_out_std_2', col_name] = outlier_ratio_2\n",
    "        outlier_ratio_df.loc['ratio_out_std_3', col_name] = outlier_ratio_3\n",
    "\n",
    "        # 2. 查找异常值\n",
    "        dict_find_2 = {'valid_time_gmt':[], col_name:[], 'raw_out':[]} # 两个标准差\n",
    "        dict_find_3 = {'valid_time_gmt':[], col_name:[], 'raw_out':[]} # 三个标准差\n",
    "        data_find = df_statistics.loc[:, ['valid_time_gmt', col_name]]\n",
    "        for i in range(len(mask_2)):\n",
    "            if mask_2[i] == 1:\n",
    "                dict_find_2['valid_time_gmt'].append(data_find.iloc[i, 0])\n",
    "                dict_find_2[col_name].append(data_find.iloc[i, 1])\n",
    "                dict_find_2['raw_out'].append(raw_out[i])\n",
    "            if mask_3[i] == 1:\n",
    "                dict_find_3['valid_time_gmt'].append(data_find.iloc[i, 0])\n",
    "                dict_find_3[col_name].append(data_find.iloc[i, 1])\n",
    "                dict_find_3['raw_out'].append(raw_out[i])\n",
    "        outlier_value_2_df = pd.DataFrame(dict_find_2).sort_values(by=[\"raw_out\", \"valid_time_gmt\"], ascending=[False, True])\n",
    "        outlier_value_3_df = pd.DataFrame(dict_find_3).sort_values(by=[\"raw_out\", \"valid_time_gmt\"], ascending=[False, True])\n",
    "        df_2_list.append(outlier_value_2_df)\n",
    "        df_3_list.append(outlier_value_3_df)\n",
    "\n",
    "        outlier_value_path_2 = f'./statistics/{c}/outlier_value_2std/'  \n",
    "        outlier_value_path_3 = f'./statistics/{c}/outlier_value_3std/'\n",
    "        outlier_value_file = c+f'Weather_ori_1_{col_name}_outlier_value.csv'\n",
    "        \n",
    "        if not os.path.exists(outlier_value_path_2):\n",
    "            os.makedirs(outlier_value_path_2)\n",
    "        if not os.path.exists(outlier_value_path_3):\n",
    "            os.makedirs(outlier_value_path_3)\n",
    "        outlier_value_2_df.to_csv(outlier_value_path_2+outlier_value_file,\\\n",
    "                          index=False)\n",
    "        outlier_value_3_df.to_csv(outlier_value_path_3+outlier_value_file,\\\n",
    "                          index=False)\n",
    "    \n",
    "    wb_2 = Workbook()\n",
    "    wb_2.save(outlier_value_path_2+c+f'_ALL_Weather_ori_1_outlier_value.xlsx')\n",
    "    wb_3 = Workbook()\n",
    "    wb_3.save(outlier_value_path_3+c+f'_ALL_Weather_ori_1_outlier_value.xlsx')\n",
    "    for df, col_name in zip(df_2_list,number_col):\n",
    "        with pd.ExcelWriter(outlier_value_path_2+c+f'_ALL_Weather_ori_1_outlier_value.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "            writer.book = wb_2\n",
    "            df.to_excel(writer, sheet_name=col_name)\n",
    "    for df, col_name in zip(df_3_list,number_col):\n",
    "        with pd.ExcelWriter(outlier_value_path_3+c+f'_ALL_Weather_ori_1_outlier_value.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "            writer.book = wb_3\n",
    "            df.to_excel(writer, sheet_name=col_name)\n",
    "\n",
    "    # writer_2 = pd.ExcelWriter(outlier_value_path_2+c+f'_ALL_Weather_ori_1_outlier_value.xlsx' ,mode='a',engine='openpyxl')\n",
    "    # writer_3 = pd.ExcelWriter(outlier_value_path_3+c+f'_ALL_Weather_ori_1_outlier_value.xlsx' ,mode='a',engine='openpyxl')\n",
    "    # for df, col_name in zip(df_2_list,number_col):\n",
    "    #     df.to_excel(writer_2, col_name)\n",
    "    # for df, col_name in zip(df_3_list,number_col):\n",
    "    #     df.to_excel(writer_3, col_name)\n",
    "    # writer_2.save()\n",
    "    # writer_2.close()\n",
    "    # writer_3.save()\n",
    "    # writer_3.close()\n",
    "\n",
    "    outlier_ratio_path = f'./statistics/{c}/'\n",
    "    outlier_ratio_file = c+'Weather_ori_1_outlier_ratio.csv'\n",
    "    if not os.path.exists(outlier_ratio_path):\n",
    "        os.makedirs(outlier_ratio_path)\n",
    "    outlier_ratio_df.to_csv(outlier_ratio_path+outlier_ratio_file,\\\n",
    "                          index=True)\n",
    "\n",
    "outlier_ratio_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 先对其进行处理，使每小时内的所有数据合并\n",
    "（1）对于非数字的列（比如天气描述，为字符串），取第一次出现的  \n",
    "（2）对于数字的列，若非‘最高气温’和‘最低气温’这两列，则取平均值  \n",
    "（3）对于最高（最低）气温，取最大（最小）值  \n",
    "（4）把expire_time替换成修改前的valid_time（valid_time即为观测的开始时间），再由（1），则处理后的expire_time，即为每小时的第一条数据的具体时间   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "# col = ['STATION', 'STATION_NAME', 'ELEVATION', 'LATITUDE', 'LONGITUDE', 'DATE', 'REPORTTYPE', 'HOURLYSKYCONDITIONS', 'HOURLYVISIBILITY', 'HOURLYPRSENTWEATHERTYPE', 'HOURLYDRYBULBTEMPF', 'HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPF', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempF', 'HOURLYDewPointTempC', 'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYWindGustSpeed', 'HOURLYStationPressure', 'HOURLYPressureTendency', 'HOURLYPressureChange', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting', 'DAILYMaximumDryBulbTemp', 'DAILYMinimumDryBulbTemp', 'DAILYAverageDryBulbTemp', 'DAILYDeptFromNormalAverageTemp', 'DAILYAverageRelativeHumidity', 'DAILYAverageDewPointTemp', 'DAILYAverageWetBulbTemp', 'DAILYHeatingDegreeDays', 'DAILYCoolingDegreeDays', 'DAILYSunrise', 'DAILYSunset', 'DAILYWeather', 'DAILYPrecip', 'DAILYSnowfall', 'DAILYSnowDepth', 'DAILYAverageStationPressure', 'DAILYAverageSeaLevelPressure', 'DAILYAverageWindSpeed', 'DAILYPeakWindSpeed', 'PeakWindDirection', 'DAILYSustainedWindSpeed', 'DAILYSustainedWindDirection', 'MonthlyMaximumTemp', 'MonthlyMinimumTemp', 'MonthlyMeanTemp', 'MonthlyAverageRH', 'MonthlyDewpointTemp', 'MonthlyWetBulbTemp', 'MonthlyAvgHeatingDegreeDays', 'MonthlyAvgCoolingDegreeDays', 'MonthlyStationPressure', 'MonthlySeaLevelPressure', 'MonthlyAverageWindSpeed', 'MonthlyTotalSnowfall', 'MonthlyDeptFromNormalMaximumTemp', 'MonthlyDeptFromNormalMinimumTemp', 'MonthlyDeptFromNormalAverageTemp', 'MonthlyDeptFromNormalPrecip', 'MonthlyTotalLiquidPrecip', 'MonthlyGreatestPrecip', 'MonthlyGreatestPrecipDate', 'MonthlyGreatestSnowfall', 'MonthlyGreatestSnowfallDate', 'MonthlyGreatestSnowDepth', 'MonthlyGreatestSnowDepthDate', 'MonthlyDaysWithGT90Temp', 'MonthlyDaysWithLT32Temp', 'MonthlyDaysWithGT32Temp', 'MonthlyDaysWithLT0Temp', 'MonthlyDaysWithGT001Precip', 'MonthlyDaysWithGT010Precip', 'MonthlyDaysWithGT1Snow', 'MonthlyMaxSeaLevelPressureValue', 'MonthlyMaxSeaLevelPressureDate', 'MonthlyMaxSeaLevelPressureTime', 'MonthlyMinSeaLevelPressureValue', 'MonthlyMinSeaLevelPressureDate', 'MonthlyMinSeaLevelPressureTime', 'MonthlyTotalHeatingDegreeDays', 'MonthlyTotalCoolingDegreeDays', 'MonthlyDeptFromNormalHeatingDD', 'MonthlyDeptFromNormalCoolingDD', 'MonthlyTotalSeasonToDateHeatingDD', 'MonthlyTotalSeasonToDateCoolingDD']\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "number_col = ['temp', 'pressure_tend', 'dewPt', 'heat_index', 'rh',\\\n",
    "              'pressure', 'vis', 'wc', 'gust',\\\n",
    "              'wspd', 'precip_total', 'precip_hrly', 'snow_hrly',\\\n",
    "              'feels_like', 'uv_index']\n",
    "special_col = ['max_temp','min_temp'] \n",
    "for c in cities_ls:\n",
    "    f_str = c + suffix # the name of the file\n",
    "    df_process = pd.read_csv(f_str)\n",
    "    num_rows = len(df_process)\n",
    "    print(c, \"Number of rows:\", num_rows)\n",
    "\n",
    "    df_process['expire_time_gmt'] = df_process['valid_time_gmt'].copy()\n",
    "    df_process['valid_time_gmt'] = df_process['valid_time_gmt'].apply(lambda x: str(x).split(':')[0])\n",
    "    \n",
    "\n",
    "    new_dict = {}\n",
    "    for col_name in col:\n",
    "        new_dict[col_name] = []\n",
    "    last_date = 0\n",
    "    tmp_dict = {}\n",
    "    same_cnt = 0\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        now_date = df_process.loc[i, 'valid_time_gmt']\n",
    "        # DATE不相同，需要处理，其中前后边界需要特别处理\n",
    "        if last_date != now_date:\n",
    "            # 先把数据的列除以相同的行数，即转换为平均值\n",
    "            for col_name in number_col:\n",
    "                if i != 0: tmp_dict[col_name] /= same_cnt\n",
    "            # 取最大or最小值\n",
    "            if i != 0: tmp_dict['max_temp'] = max(tmp_dict['max_temp'])\n",
    "            if i != 0: tmp_dict['min_temp'] = min(tmp_dict['min_temp'])\n",
    "            # 然后把tmp_dict插入new_dict中\n",
    "            for col_name in col:\n",
    "                if i != 0 or i == num_rows-1: \n",
    "                    new_dict[col_name].append(tmp_dict[col_name])\n",
    "            # 并初始化新的tmp_dict\n",
    "            for col_name in col:\n",
    "                tmp_dict[col_name] = df_process.loc[i, col_name]\n",
    "            for col_name in number_col:\n",
    "                tmp_dict[col_name] = float(tmp_dict[col_name])\n",
    "            for col_name in special_col:\n",
    "                tmp_dict[col_name] = [float(tmp_dict[col_name])]\n",
    "                \n",
    "\n",
    "            # 开始新的一轮相同DATE的行的处理，初始化↓\n",
    "            same_cnt = 1\n",
    "            last_date = now_date\n",
    "\n",
    "        # DATE相同，加入到tmp_dict中\n",
    "        else:\n",
    "            # 计数加1\n",
    "            same_cnt += 1\n",
    "            # 直接加到tmo_dict（求和），最后再进行求平均值\n",
    "            # 对于不在number_col中的列，其为字符串无法取平均值，只保留第一个的数据，即上方初始化的值\n",
    "            for col_name in number_col:\n",
    "                tmp_dict[col_name] += float(df_process.loc[i, col_name])\n",
    "            # 对于最大值or最小值，先加入list中，后续再处理\n",
    "            for col_name in special_col:\n",
    "                tmp_dict[col_name].append(float(df_process.loc[i, col_name]))\n",
    "\n",
    "            # 特殊处理尾端\n",
    "            if i == num_rows-1:\n",
    "                # 先转化为平均值\n",
    "                for col_name in number_col:\n",
    "                    tmp_dict[col_name] /= same_cnt\n",
    "                # 取最大or最小值\n",
    "                tmp_dict['max_temp'] = max(tmp_dict['max_temp'])\n",
    "                tmp_dict['min_temp'] = min(tmp_dict['min_temp'])\n",
    "                # 然后把tmp_dict插入new_dict中\n",
    "                for col_name in col:\n",
    "                    new_dict[col_name].append(tmp_dict[col_name])\n",
    "\n",
    "    new_df = pd.DataFrame(new_dict)\n",
    "    process_path = f'./preprocessed/step1/{c}/'\n",
    "    process_file = c+'Weather_1h.csv'\n",
    "    if not os.path.exists(process_path):\n",
    "        os.makedirs(process_path)\n",
    "    new_df.to_csv(process_path+process_file, index=False)\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 处理一下wx_phrase列，使其字段少一些\n",
    "比如合并partly cloudy、mostly cloudy统一为cloudy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 先打印出来，看一下有哪些字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "dict = {}\n",
    "for c in cities_ls:\n",
    "    f_path = f'./preprocessed/step1/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_wx_phrase = list(pd.read_csv(f_str)['wx_phrase'].value_counts().index)\n",
    "    num_rows = len(df_wx_phrase)\n",
    "    print(c, \"Number of types of wx_phrase:\", num_rows)\n",
    "    if num_rows == 0: continue\n",
    "    print(df_wx_phrase[:5])\n",
    "    dict[c] = df_wx_phrase\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 统计一下非重复的类别有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "big_ls = []\n",
    "for c in cities_ls:\n",
    "    tmp_ls = dict[c]\n",
    "    new_ls = []\n",
    "    for s in tmp_ls:\n",
    "        s_ls = s.split(' / ')\n",
    "        for t in s_ls:\n",
    "            if t not in new_ls: new_ls.append(t)\n",
    "            if t not in big_ls: big_ls.append(t)\n",
    "    print(c, \"Number of types of wx_phrase:\", len(new_ls))\n",
    "    if len(new_ls) == 0: continue\n",
    "    print(new_ls[:5])\n",
    "    new_dict[c] = new_ls\n",
    "\n",
    "print(len(big_ls))\n",
    "for s in big_ls:\n",
    "    print('- ' + s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 列出所有出现的wx_phrase字段，并对其进行转换使其更加精简"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于转换的字典\n",
    "trans_dict = \\\n",
    "{\n",
    "    'Cloudy': 'Cloudy',\n",
    "    'Mostly Cloudy': 'Cloudy',\n",
    "    'Fair': 'Fair',\n",
    "    'Partly Cloudy': 'Cloudy',\n",
    "    'Light Rain': 'Rainy',\n",
    "    'Fog': 'Foggy',\n",
    "    'Light Snow': 'Snowy',\n",
    "    'Windy': 'Windy',\n",
    "    'Rain': 'Rainy',\n",
    "    'Light Drizzle': 'Rainy',\n",
    "    'Heavy Rain': 'Rainy',\n",
    "    'Haze': 'Overcast',\n",
    "    'Light Rain with Thunder': 'Thunderstorm',\n",
    "    'Snow': 'Snowy',\n",
    "    'Heavy T-Storm': 'Thunderstorm',\n",
    "    'Wintry Mix': 'Snowy',\n",
    "    'Thunder': 'Thunderstorm',\n",
    "    'T-Storm': 'Thunderstorm',\n",
    "    'Drizzle and Fog': 'Foggy',\n",
    "    'Freezing Rain': 'Rainy',\n",
    "    'Thunder in the Vicinity': 'Thunderstorm',\n",
    "    'Rain and Snow': 'Rainy / Snowy',\n",
    "    'Heavy Snow': 'Snowy',\n",
    "    'Rain and Sleet': 'Rainy',\n",
    "    'Snow and Sleet': 'Snowy',\n",
    "    'Light Freezing Rain': 'Rainy',\n",
    "    'Light Freezing Drizzle': 'Rainy',\n",
    "    'Light Snow and Sleet': 'Snowy',\n",
    "    'Light Sleet': 'Snowy',\n",
    "    'Patches of Fog': 'Foggy',\n",
    "    'Mist': 'Foggy',\n",
    "    'Sleet': 'Snowy',\n",
    "    'Shallow Fog': 'Foggy',\n",
    "    'Unknown Precipitation': 'Overcast',\n",
    "    'Smoke': 'Overcast',\n",
    "    'Drizzle': 'Rainy',\n",
    "    'Blowing Snow': 'Snowy',\n",
    "    'Funnel Cloud': 'Tornado',\n",
    "    'Widespread Dust': 'Overcast',\n",
    "    'Tornado': 'Tornado',\n",
    "    'Thunder and Hail': 'Thunderstorm / Hail',\n",
    "    'Heavy Sleet': 'Snowy',\n",
    "    'Squalls': 'Thunderstorm',\n",
    "    'Light Rain Shower': 'Rainy',\n",
    "    'Showers in the Vicinity': 'Rainy',\n",
    "    'Rain Shower': 'Rainy',\n",
    "    'Heavy Rain Shower': 'Rainy',\n",
    "    'Partial Fog': 'Foggy',\n",
    "    'Heavy Drizzle': 'Rainy',\n",
    "    'Thunder and Small Hail': 'Thunderstorm / Hail',\n",
    "    'Low Drifting Dust': 'Overcast',\n",
    "    'Small Hail': 'Hail',\n",
    "    'Sand': 'Overcast',\n",
    "    'Dust Whirlwinds': 'Overcast',\n",
    "    'Hail': 'Hail'\n",
    "}\n",
    "error_ls = []\n",
    "v_ls = []\n",
    "for s in big_ls:\n",
    "    if s not in trans_dict.keys():\n",
    "        error_ls.append()\n",
    "print(f'number of errors is {len(error_ls)}')\n",
    "for v in trans_dict.values():\n",
    "    if v not in v_ls: v_ls.append(v)\n",
    "print(f'number of new types of wx_phrase is {len(v_ls)}')\n",
    "for v in v_ls: print(v, end='  ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 one-hot编码替换原始的wx_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_type = ['Cloudy', 'Fair', 'Rainy', 'Foggy', 'Snowy',\\\n",
    "             'Windy', 'Overcast', 'Thunderstorm', 'Tornado', 'Hail']\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "dict = {}\n",
    "test_ls = ['Cloudy / Fair', 'Rain / Widespread Dust', 'Heavy Sleet / Thunder and Hail']\n",
    "# special_ls = ['Rain and Snow', 'Thunder and Hail', 'Thunder and Small Hail']\n",
    "def trans(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        # print(x)\n",
    "        return '0000000000'\n",
    "    tmp_ls = []\n",
    "    tmp_str = ''\n",
    "    tmp = x.split(' / ')\n",
    "    for t in tmp:\n",
    "        t = trans_dict[t] \n",
    "        if '/' in t:\n",
    "            t = t.split(' / ')\n",
    "            for t1 in t:\n",
    "                tmp_ls.append(t1)\n",
    "        else:\n",
    "            tmp_ls.append(t)\n",
    "    for type in all_type:\n",
    "        tmp_str += '1' if type in tmp_ls else '0'\n",
    "    return tmp_str\n",
    "    # return tmp_str, tmp_ls\n",
    "\n",
    "for test in test_ls:\n",
    "    # tmp_str, tmp_ls = trans(test)\n",
    "    # print(test + ':', tmp_str, tmp_ls)\n",
    "    tmp_str = trans(test)\n",
    "    print(test + ':', tmp_str)\n",
    "    \n",
    "for c in cities_ls:\n",
    "    f_path = f'./preprocessed/step1/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_trans = pd.read_csv(f_str)\n",
    "    num_rows = len(df_trans)\n",
    "    print(c, \"Number of rows:\", num_rows)\n",
    "    if num_rows == 0: continue\n",
    "    df_trans['wx_phrase'] = df_trans['wx_phrase'].apply(trans)\n",
    "    trans_path = f'./preprocessed/step2/{c}/'\n",
    "    trans_file = c+'Weather_1h.csv'\n",
    "    if not os.path.exists(trans_path):\n",
    "        os.makedirs(trans_path)\n",
    "    \n",
    "    def date2str(date_str: str):\n",
    "        from datetime import datetime\n",
    "        import re\n",
    "        ls = re.split(r'/| ', date_str)\n",
    "        s = f'{ls[0]}{ls[1].zfill(2)}{ls[2].zfill(2)}{ls[3].zfill(2)}'\n",
    "        return s\n",
    "\n",
    "    df_trans['valid_time_gmt'] = df_trans['valid_time_gmt'].apply(date2str)\n",
    "\n",
    "    df_trans.to_csv(trans_path+trans_file, index=False)\n",
    "df_trans\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.填补缺失数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "all_type = ['Cloudy', 'Fair', 'Rainy', 'Foggy', 'Snowy',\\\n",
    "             'Windy', 'Overcast', 'Thunderstorm', 'Tornado', 'Hail']\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "dict = {}\n",
    "    \n",
    "for c in cities_ls:\n",
    "    f_path = f'./preprocessed/step2/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_for_fill = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "    num_rows = len(df_for_fill)\n",
    "    print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "    print(print_msg.ljust(50), end='')\n",
    "    if num_rows == 0: continue\n",
    "\n",
    "    def str2date(s:str)->datetime.datetime:\n",
    "        \"\"\"s: s must like '2023051102', its first four letters represent year, \n",
    "        and then each 2 letters represent month, day, hour respectively\"\"\"\n",
    "        s = str(s)\n",
    "        year, month, day, hour = s[:4], s[4:6], s[6:8], s[8:]\n",
    "        return datetime.datetime(int(year), int(month), int(day), int(hour))\n",
    "    def date2str(date:datetime.datetime)->str:\n",
    "        # {ls[0]}{ls[1].zfill(2)}{ls[2].zfill(2)}{ls[3].zfill(2)}\n",
    "        return f'{str(date.year)}{str(date.month).zfill(2)}{str(date.day).zfill(2)}{str(date.hour).zfill(2)}'\n",
    "\n",
    "    start_str = df_for_fill.loc[0, 'valid_time_gmt']\n",
    "    # print(start_str)\n",
    "    end_str = df_for_fill.loc[num_rows-1, 'valid_time_gmt']\n",
    "    start_date = str2date(start_str)\n",
    "    end_date = str2date(end_str)\n",
    "    now_date = start_date\n",
    "    cnt = 0 # 用于索引csv的行\n",
    "    dict_for_df = {} # 用于构建新的df\n",
    "    ls_filled = [] # 用于记录哪些时刻原本是没有的而是被新填充的\n",
    "    for col_name in col:\n",
    "        dict_for_df[col_name] = []\n",
    "    while now_date <= end_date:\n",
    "        csv_str = df_for_fill.loc[cnt, 'valid_time_gmt']\n",
    "        csv_date = str2date(csv_str)\n",
    "        if csv_date == now_date:\n",
    "            tmp_row = df_for_fill.loc[cnt].copy() # 不需要填充\n",
    "            for col_name in col:\n",
    "                dict_for_df[col_name].append(tmp_row[col_name])\n",
    "        else:\n",
    "            tmp_row = df_for_fill.loc[cnt-1].copy() # 取上一时刻的来填充\n",
    "            tmp_str = date2str(now_date)\n",
    "            tmp_row['valid_time_gmt'] = tmp_str # 修改时间\n",
    "            ls_filled.append(tmp_str)\n",
    "            for col_name in col:\n",
    "                dict_for_df[col_name].append(tmp_row[col_name])\n",
    "\n",
    "            cnt -= 1 # 这个是为了防止跳过当前的行\n",
    "        now_date += datetime.timedelta(hours=1)\n",
    "        cnt += 1\n",
    "\n",
    "    df_filled = pd.DataFrame(dict_for_df) # 生成新的dataframe\n",
    "    df_record = pd.DataFrame(ls_filled) # 记录哪些时刻是被新填充的\n",
    "    filled_path = f'./preprocessed/step3/{c}/'\n",
    "    filled_file = c+'Weather_1h.csv'\n",
    "    record_file = c+'record.csv'\n",
    "\n",
    "    if not os.path.exists(filled_path):\n",
    "        os.makedirs(filled_path)\n",
    "\n",
    "    df_filled.to_csv(filled_path+filled_file, index=False)\n",
    "    df_record.to_csv(filled_path+record_file, index=False)\n",
    "    print(f'after filled: {len(df_filled)}')\n",
    "df_filled\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 构建场景"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1. 分析一下总的各类天气的出现频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "all_type = ['Cloudy', 'Fair', 'Rainy', 'Foggy', 'Snowy',\\\n",
    "             'Windy', 'Overcast', 'Thunderstorm', 'Tornado', 'Hail']\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "dict = {}\n",
    "    \n",
    "for index in range(len(all_type)):\n",
    "    print('-'*40)\n",
    "    print(all_type[index])\n",
    "    for c in cities_ls:\n",
    "        f_path = f'./preprocessed/step3/{c}/'\n",
    "        f_file = c+'Weather_1h.csv' \n",
    "        f_str = f_path + f_file # the name of the file\n",
    "        df_for_rain = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "        num_rows = len(df_for_rain)\n",
    "        print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "        print(print_msg.ljust(50), end='')\n",
    "        if num_rows == 0: continue\n",
    "\n",
    "        # 先按照7:1:2把原数据分成三个部分，统计下各部分雨天的比例\n",
    "        cnt1 = 0 # 7\n",
    "        cnt2 = 0 # 1\n",
    "        cnt3 = 0 # 2\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            wx_phrase = str(df_for_rain.loc[i, 'wx_phrase'])\n",
    "            if wx_phrase[index] == '1' or\\\n",
    "                (index == 2 and float(df_for_rain.loc[i, 'precip_hrly']) > 0) or\\\n",
    "                (index == 4 and float(df_for_rain.loc[i, 'snow_hrly']) > 0) or\\\n",
    "                (index == 0 and str(df_for_rain.loc[i, 'clds']) in ['SCT','BKN','OVC']):\n",
    "                if i < num_rows*7/10: cnt1 += 1\n",
    "                if i >= num_rows*7/10 and i <num_rows*8/10: cnt2 += 1\n",
    "                if i >= num_rows*8/10: cnt3 += 1\n",
    "        print(f'{cnt1}'.ljust(10)+f'{cnt2}'.ljust(10)+f'{cnt3}'.ljust(10), f'{all_type[index]} ratio of all: {(cnt1+cnt2+cnt3)/num_rows:.4f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 滑动窗口分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "all_type = ['Cloudy', 'Fair', 'Rainy', 'Foggy', 'Snowy',\\\n",
    "             'Windy', 'Overcast', 'Thunderstorm', 'Tornado', 'Hail']\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "dict = {}\n",
    "window_size = 200 * 24 # 目前构建train、val、test总共200天的场景 \n",
    "window_step = 10 * 24 # 目前设置窗口每次滑动10天\n",
    "    \n",
    "\n",
    "for c in cities_ls:\n",
    "    # BAY Number of rows: 4367\n",
    "    # LA Number of rows:  2879\n",
    "    # BAY 和 LA 数据集小，构建场景也因此窗口大小需要小一些\n",
    "    if c in ['BAY','LA']:\n",
    "        window_size = 100 * 24 # 100天\n",
    "        window_step = 5 * 24 # 5天\n",
    "    else:\n",
    "        window_size = 200 * 24 # 目前构建train、val、test总共200天的场景 \n",
    "        window_step = 10 * 24 # 目前设置窗口每次滑动10天\n",
    "    f_path = f'./preprocessed/step3/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_for_window = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "    num_rows = len(df_for_window)\n",
    "    print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "    print(print_msg.ljust(50), end='\\n')\n",
    "    if num_rows == 0: continue\n",
    "\n",
    "    now_index = 0 \n",
    "    window_cnt = 0 # 记录窗口滑动次数\n",
    "    cnt_max = 1000 # 滑动次数阈值，目前置为1000，能覆盖近30年，当滑动次数大于cnt_max时，停止循环\n",
    "    while now_index + window_size <= num_rows and window_cnt < cnt_max: \n",
    "        cnt = {} # 用于计数train, val, test中各类天气的出现频次\n",
    "        # 初始化\n",
    "        # cnt['vis'] = [0,0,0]\n",
    "        for index in range(len(all_type)-2): # -2是不考虑最后的tornado和hail，因它们出现过少\n",
    "            cnt[all_type[index]] = [0,0,0] \n",
    "        for i in range(window_size):\n",
    "            j = now_index + i \n",
    "            # if j >= num_rows: break\n",
    "            wx_phrase = str(df_for_window.loc[j, 'wx_phrase'])\n",
    "            # vis = float(df_for_window.loc[j, 'vis']) if df_for_window.loc[j, 'vis'] >= 0 else 0\n",
    "            # if i < window_size*7/10: cnt['vis'][0] += vis\n",
    "            # if i >= window_size*7/10 and i <window_size*9/10: cnt['vis'][1] += vis\n",
    "            # if i >= window_size*9/10: cnt['vis'][2] += vis\n",
    "            for index in range(len(all_type)-2): # -2是不考虑最后的tornado和hail，因它们出现过少\n",
    "                if wx_phrase[index] == '1' or\\\n",
    "                    (index == 2 and float(df_for_window.loc[j, 'precip_hrly']) > 0) or\\\n",
    "                    (index == 4 and float(df_for_window.loc[j, 'snow_hrly']) > 0) or\\\n",
    "                    (index == 0 and str(df_for_window.loc[j, 'clds']) in ['SCT','BKN','OVC']):\n",
    "                    if i < window_size*7/10: cnt[all_type[index]][0] += 1\n",
    "                    if i >= window_size*7/10 and i <window_size*8/10: cnt[all_type[index]][1] += 1\n",
    "                    if i >= window_size*8/10: cnt[all_type[index]][2] += 1\n",
    "        cnt_df = pd.DataFrame(cnt, index=['Training', 'Validatation', 'Test'])\n",
    "        cnt_df.loc['Training'] /= window_size*7/10\n",
    "        cnt_df.loc['Validatation'] /= window_size*1/10\n",
    "        cnt_df.loc['Test'] /= window_size*2/10\n",
    "\n",
    "        \n",
    "        cnt_df.plot.bar(figsize=(16,9))\n",
    "\n",
    "        plt.xticks(rotation=0, fontsize = 'xx-large')\n",
    "        plt.title(f\"{c} ({df_for_window.loc[now_index, 'valid_time_gmt']} - {df_for_window.loc[now_index+window_size, 'valid_time_gmt']})\", fontsize=32)\n",
    "        # plt.show()\n",
    "        png_path = f'./scene/general/{c}/'\n",
    "        png_file = f\"{c} ({df_for_window.loc[now_index, 'valid_time_gmt']} - {df_for_window.loc[now_index+window_size, 'valid_time_gmt']})\"\n",
    "        if not os.path.exists(png_path):\n",
    "            os.makedirs(png_path)\n",
    "        plt.savefig(png_path+png_file, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "        now_index += window_step\n",
    "        window_cnt += 1\n",
    "cnt_df\n",
    "\n",
    "# cnt_df.plot(figsize=(16,9))\n",
    "\n",
    "# cnt_df.plot.bar(figsize=(16,9))\n",
    "\n",
    "# plt.xticks(rotation=0)\n",
    "# # plt.show()\n",
    "# plt.savefig('./tmp.png', dpi=120)\n",
    "# plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 季节变迁\n",
    "需要把窗口调小，考虑为调成100天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "all_type = ['Cloudy', 'Fair', 'Rainy', 'Foggy', 'Snowy',\\\n",
    "             'Windy', 'Overcast', 'Thunderstorm', 'Tornado', 'Hail']\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "dict = {}\n",
    "window_size = 100 * 24 # 目前构建train、val、test总共100天的场景 \n",
    "window_step = 5 * 24 # 目前设置窗口每次滑动5天\n",
    "    \n",
    "\n",
    "for c in cities_ls:\n",
    "    # BAY Number of rows: 4367\n",
    "    # LA Number of rows:  2879\n",
    "    # BAY 和 LA 数据集小，构建场景也因此窗口大小需要小一些\n",
    "    if c in ['BAY','LA']:\n",
    "        window_size = 60 * 24 # 100天\n",
    "        window_step = 3 * 24 # 5天\n",
    "    else:\n",
    "        window_size = 100 * 24 # 目前构建train、val、test总共100天的场景 \n",
    "        window_step = 5 * 24 # 目前设置窗口每次滑动5天\n",
    "    f_path = f'./preprocessed/step3/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_for_window = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "    num_rows = len(df_for_window)\n",
    "    print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "    print(print_msg.ljust(50), end='\\n')\n",
    "    if num_rows == 0: continue\n",
    "\n",
    "    now_index = 0 \n",
    "    window_cnt = 0 # 记录窗口滑动次数\n",
    "    cnt_max = 1000 # 滑动次数阈值，目前置为1000，能覆盖近30年，当滑动次数大于cnt_max时，停止循环\n",
    "    while now_index + window_size <= num_rows and window_cnt < cnt_max: \n",
    "        cnt = {} # 用于计数train, val, test中各类天气的出现频次\n",
    "        temp = [0,0,0] # 用于记录温度\n",
    "        cnt_temp = [0,0,0]\n",
    "        # 初始化\n",
    "        # cnt['vis'] = [0,0,0]\n",
    "        for index in range(len(all_type)-2): # -2是不考虑最后的tornado和hail，因它们出现过少\n",
    "            cnt[all_type[index]] = [0,0,0] \n",
    "        for i in range(window_size):\n",
    "            j = now_index + i \n",
    "            # if j >= num_rows: break\n",
    "            wx_phrase = str(df_for_window.loc[j, 'wx_phrase'])\n",
    "            feels_like = df_for_window.loc[j, 'feels_like']\n",
    "            # vis = float(df_for_window.loc[j, 'vis']) if df_for_window.loc[j, 'vis'] >= 0 else 0\n",
    "            # if i < window_size*7/10: cnt['vis'][0] += vis\n",
    "            # if i >= window_size*7/10 and i <window_size*9/10: cnt['vis'][1] += vis\n",
    "            # if i >= window_size*9/10: cnt['vis'][2] += vis\n",
    "            for index in range(len(all_type)-2): # -2是不考虑最后的tornado和hail，因它们出现过少\n",
    "                if wx_phrase[index] == '1' or\\\n",
    "                    (index == 2 and float(df_for_window.loc[j, 'precip_hrly']) > 0) or\\\n",
    "                    (index == 4 and float(df_for_window.loc[j, 'snow_hrly']) > 0) or\\\n",
    "                    (index == 0 and str(df_for_window.loc[j, 'clds']) in ['SCT','BKN','OVC']):\n",
    "                    if i < window_size*7/10: cnt[all_type[index]][0] += 1\n",
    "                    if i >= window_size*7/10 and i <window_size*8/10: cnt[all_type[index]][1] += 1\n",
    "                    if i >= window_size*8/10: cnt[all_type[index]][2] += 1\n",
    "            if not pd.isnull(feels_like):\n",
    "                if i < window_size*7/10: \n",
    "                    cnt_temp[0] += 1\n",
    "                    temp[0] += float(feels_like)\n",
    "                if i >= window_size*7/10 and i <window_size*8/10: \n",
    "                    cnt_temp[1] += 1\n",
    "                    temp[1] += float(feels_like)\n",
    "                if i >= window_size*8/10: \n",
    "                    cnt_temp[2] += 1\n",
    "                    temp[2] += float(feels_like)\n",
    "        cnt_df = pd.DataFrame(cnt, index=['Training', 'Validatation', 'Test'])\n",
    "        cnt_df.loc['Training'] /= window_size*7/10\n",
    "        cnt_df.loc['Validatation'] /= window_size*1/10\n",
    "        cnt_df.loc['Test'] /= window_size*2/10\n",
    "\n",
    "        temp_df = pd.DataFrame(temp, index=['Training', 'Validatation', 'Test'])\n",
    "        temp_df.loc['Training'] /= cnt_temp[0]\n",
    "        temp_df.loc['Validatation'] /= cnt_temp[1]\n",
    "        temp_df.loc['Test'] /= cnt_temp[2]\n",
    "\n",
    "        \n",
    "        cnt_df.plot(figsize=(16,9),kind='bar')\n",
    "        temp_df.iloc[:,0].plot(figsize=(16,9), secondary_y=True)\n",
    "        for i in range(len(temp_df)):\n",
    "            plt.annotate(temp_df.iloc[i,0], (i, temp_df.iloc[i,0]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "        plt.xticks(rotation=0, fontsize = 'xx-large')\n",
    "        plt.title(f\"{c} ({df_for_window.loc[now_index, 'valid_time_gmt']} - {df_for_window.loc[now_index+window_size, 'valid_time_gmt']})\", fontsize=32)\n",
    "        # plt.show()\n",
    "        png_path = f'./scene/season_7_1_2/{c}/'\n",
    "        png_file = f\"{c} ({df_for_window.loc[now_index, 'valid_time_gmt']} - {df_for_window.loc[now_index+window_size, 'valid_time_gmt']})\"\n",
    "        if not os.path.exists(png_path):\n",
    "            os.makedirs(png_path)\n",
    "        plt.savefig(png_path+png_file, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "        now_index += window_step\n",
    "        window_cnt += 1\n",
    "cnt_df\n",
    "\n",
    "# cnt_df.plot(figsize=(16,9))\n",
    "\n",
    "# cnt_df.plot.bar(figsize=(16,9))\n",
    "\n",
    "# plt.xticks(rotation=0)\n",
    "# # plt.show()\n",
    "# plt.savefig('./tmp.png', dpi=120)\n",
    "# plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3 将设定日期的天气数据取出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "col = ['key', 'class', 'expire_time_gmt', 'obs_id', 'obs_name', 'valid_time_gmt', \n",
    "         'day_ind', 'temp', 'wx_icon', 'icon_extd', 'wx_phrase', 'pressure_tend', \n",
    "         'pressure_desc', 'dewPt', 'heat_index', 'rh', 'pressure', 'vis', 'wc', \n",
    "         'wdir', 'wdir_cardinal', 'gust', 'wspd', 'max_temp', 'min_temp', \n",
    "         'precip_total', 'precip_hrly', 'snow_hrly', 'uv_desc', 'feels_like', \n",
    "         'uv_index', 'qualifier', 'qualifier_svrty', 'blunt_phrase', 'terse_phrase', \n",
    "         'clds', 'water_temp', 'primary_wave_period', 'primary_wave_height', \n",
    "         'primary_swell_period', 'primary_swell_height', 'primary_swell_direction', \n",
    "         'secondary_swell_period', 'secondary_swell_height', 'secondary_swell_direction']\n",
    "all_type = ['Cloudy', 'Fair', 'Rainy', 'Foggy', 'Snowy',\\\n",
    "             'Windy', 'Overcast', 'Thunderstorm', 'Tornado', 'Hail']\n",
    "cities_ls = ['NYC', 'BAY','Chicago','DC','LA','Melbourne']\n",
    "suffix = 'Weather_ori_1.csv'\n",
    "\n",
    "def str2date(s:str)->datetime.datetime:\n",
    "        \"\"\"s: s must like '2023051102', its first four letters represent year, \n",
    "        and then each 2 letters represent month, day, hour respectively\"\"\"\n",
    "        s = str(s)\n",
    "        year, month, day, hour = s[:4], s[4:6], s[6:8], s[8:]\n",
    "        return datetime.datetime(int(year), int(month), int(day), int(hour))\n",
    "def date2str(date:datetime.datetime)->str:\n",
    "    # {ls[0]}{ls[1].zfill(2)}{ls[2].zfill(2)}{ls[3].zfill(2)}\n",
    "    return f'{str(date.year)}{str(date.month).zfill(2)}{str(date.day).zfill(2)}{str(date.hour).zfill(2)}'\n",
    "\n",
    "# 1.通用场景\n",
    "dict_1={} \n",
    "# [start_time, end_time]\n",
    "dict_1['BAY']=[2017012100, 2017050100]\n",
    "dict_1['Chicago']=[2014092323, 2015041123]\n",
    "dict_1['DC']=[2014091323, 2015040123]\n",
    "dict_1['LA']=[2012031523, 2012062323]\n",
    "dict_1['Melbourne']=[2020040300, 2020102000]\n",
    "dict_1['NYC']=[2014100323, 2015042123]\n",
    "for c in cities_ls:\n",
    "    f_path = f'./preprocessed/step3/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_for_scene = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "    num_rows = len(df_for_scene)\n",
    "    print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "    print(print_msg.ljust(50), end='\\n')\n",
    "    if num_rows == 0: continue\n",
    "\n",
    "    new_df_ls = []\n",
    "    start_time = dict_1[c][0]\n",
    "    end_time = dict_1[c][1]\n",
    "    for i in range(num_rows):\n",
    "        now_time = df_for_scene.loc[i, 'valid_time_gmt']\n",
    "        if now_time >= start_time and now_time <= end_time:\n",
    "             new_df_ls.append(df_for_scene.loc[i])\n",
    "    \n",
    "    scene_path = f'./scene/general_7_1_2/select/'\n",
    "    scene_file = f\"{c} ({start_time} - {end_time}).csv\"\n",
    "    if not os.path.exists(scene_path):\n",
    "        os.makedirs(scene_path)\n",
    "\n",
    "    new_df = pd.DataFrame(new_df_ls)\n",
    "    new_df.to_csv(scene_path+scene_file, index=False)\n",
    "\n",
    "# 2.季节变迁\n",
    "dict_2={} \n",
    "# [start_time, end_time]\n",
    "dict_2['BAY']=[2017012200, 2017032300]\n",
    "dict_2['Chicago']=[2014092823, 2015010623]\n",
    "dict_2['DC']=[2014122223, 2015040123]\n",
    "dict_2['LA']=[2012022923, 2012042923]\n",
    "dict_2['Melbourne']=[2014100700, 2015011500]\n",
    "dict_2['NYC']=[2014063023, 2014100823]\n",
    "for c in cities_ls:\n",
    "    f_path = f'./preprocessed/step3/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_for_scene = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "    num_rows = len(df_for_scene)\n",
    "    print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "    print(print_msg.ljust(50), end='\\n')\n",
    "    if num_rows == 0: continue\n",
    "\n",
    "    new_df_ls = []\n",
    "    start_time = dict_2[c][0]\n",
    "    end_time = dict_2[c][1]\n",
    "    for i in range(num_rows):\n",
    "        now_time = df_for_scene.loc[i, 'valid_time_gmt']\n",
    "        if now_time >= start_time and now_time <= end_time:\n",
    "             new_df_ls.append(df_for_scene.loc[i])\n",
    "    \n",
    "    scene_path = f'./scene/season_7_1_2/select/'\n",
    "    scene_file = f\"{c} ({start_time} - {end_time}).csv\"\n",
    "    if not os.path.exists(scene_path):\n",
    "        os.makedirs(scene_path)\n",
    "\n",
    "    new_df = pd.DataFrame(new_df_ls)\n",
    "    new_df.to_csv(scene_path+scene_file, index=False)\n",
    "\n",
    "# 3.极端天气\n",
    "dict_3={} \n",
    "# [start_time, end_time]\n",
    "dict_3['BAY']=[2017012200, 2017032300]\n",
    "dict_3['Chicago']=[2017013023, 2017081823]\n",
    "dict_3['DC']=[2017020923, 2017082823]\n",
    "dict_3['LA']=[2012022923, 2012042923]\n",
    "dict_3['Melbourne']=[2021081600, 2022030400]\n",
    "dict_3['NYC']=[2014021523, 2014090323]\n",
    "for c in cities_ls:\n",
    "    f_path = f'./preprocessed/step3/{c}/'\n",
    "    f_file = c+'Weather_1h.csv' \n",
    "    f_str = f_path + f_file # the name of the file\n",
    "    df_for_scene = pd.read_csv(f_str, converters={'wx_phrase':str})\n",
    "    num_rows = len(df_for_scene)\n",
    "    print_msg = str(c+\" Number of rows: \").ljust(30)+str(num_rows)\n",
    "    print(print_msg.ljust(50), end='\\n')\n",
    "    if num_rows == 0: continue\n",
    "\n",
    "    new_df_ls = []\n",
    "    start_time = dict_3[c][0]\n",
    "    end_time = dict_3[c][1]\n",
    "    for i in range(num_rows):\n",
    "        now_time = df_for_scene.loc[i, 'valid_time_gmt']\n",
    "        if now_time >= start_time and now_time <= end_time:\n",
    "             new_df_ls.append(df_for_scene.loc[i])\n",
    "    \n",
    "    scene_path = f'./scene/ExtremeWeather_7_1_2/'\n",
    "    scene_file = f\"{c} ({start_time} - {end_time}).csv\"\n",
    "    if not os.path.exists(scene_path):\n",
    "        os.makedirs(scene_path)\n",
    "\n",
    "    new_df = pd.DataFrame(new_df_ls)\n",
    "    new_df.to_csv(scene_path+scene_file, index=False)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雪天数据非常少，只有NYC, Chicago, DC 这三个城市的天气数据集中存在雪天，其中Chicago的雪天比例最高  \n",
    "若要构建雪天相关的场景，比如从秋到冬，则选取Chicago比较合适"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 分析天气与flow的相关性 [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "all_df = pd.read_pickle('./Public_Datasets/Bike/60_minutes/Bike_Chicago.pkl')\n",
    "print(all_df)\n",
    "print('------------------------------------')\n",
    "flow_df = all_df['Node']['TrafficNode']\n",
    "print(np.shape(flow_df))\n",
    "flow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('./preprocessed/step2/Chicago/ChicagoWeather_1h.csv')\n",
    "weather_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下无用，就是临时代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "# 定义开始日期和结束日期\n",
    "start_date = date(2013, 7, 1)\n",
    "end_date = date(2017, 9, 30)\n",
    "\n",
    "# 计算日期差\n",
    "delta = end_date - start_date\n",
    "\n",
    "# 输出天数\n",
    "print(\"Days between start and end dates:\", delta.days)\n",
    "\n",
    "# 计算小时数\n",
    "start_datetime = datetime(2009, 5, 1, 0)\n",
    "tmp_datetime = datetime(2013, 7, 1, 0)\n",
    "end_datetime = datetime(2022, 3, 31, 23)\n",
    "delta_hours = (end_datetime - start_datetime).total_seconds() / 3600\n",
    "\n",
    "# 输出小时数\n",
    "print(\"Hours between start and end dates:\", delta_hours)\n",
    "print((tmp_datetime - start_datetime).total_seconds() / 3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'num':[5,'A',7,8], 'alpha':['ASF','13TG',778, 13]})\n",
    "print(df)\n",
    "ls = []\n",
    "for row in df.values:\n",
    "    ls.append(row)\n",
    "print(ls)\n",
    "print(pd.DataFrame(ls, columns=['num', 'alpha']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'abc'\n",
    "n = 'dagvb'\n",
    "ls = [134,'aegb','erq','abc',340]\n",
    "print(s in ls, n in ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建示例数据\n",
    "data = {'月份': ['一月', '二月', '三月', '四月', '五月'],\n",
    "        '销售额': [100, 150, 120, 180, 200],}\n",
    "data2={'利润率': [0.1, 0.15, 0.12, 0.18, 0.2]}\n",
    "df = pd.DataFrame(data)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# 绘制柱状图\n",
    "df.plot(kind='bar', color='blue', alpha=0.7)\n",
    "\n",
    "# 创建第二个y轴\n",
    "ax2 = df2.iloc[:,0].plot(secondary_y=True, color='red')\n",
    "\n",
    "# 设置坐标轴标签\n",
    "plt.ylabel('销售额')\n",
    "ax2.set_ylabel('利润率')\n",
    "\n",
    "# 设置图例\n",
    "plt.legend(['销售额', '利润率'])\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}